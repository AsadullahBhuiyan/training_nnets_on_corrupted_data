% Minimal theory sketch for MNIST corruption experiment.
\documentclass[11pt]{article}
\usepackage{amsmath, amssymb}
\usepackage{geometry}
\geometry{margin=1in}

\title{Sketch: Accuracy vs. Corruption Probability in Pixel Replacement}
\author{}
\date{}

\begin{document}
\maketitle

\section*{Setup}
Let $x \in [0,1]^d$ be an input image and $y \in \{1,\dots,10\}$ the label.
We define a corruption operator $\mathcal{C}_p$ that independently replaces each pixel with
probability $p$:
\[
(\mathcal{C}_p(x))_i =
\begin{cases}
u_i, & \text{with probability } p,\\
x_i, & \text{with probability } 1-p,
\end{cases}
\quad u_i \sim \mathrm{Uniform}(0,1).
\]
Training uses corrupted inputs $\tilde{x} = \mathcal{C}_p(x)$ and clean labels $y$.
We study the test accuracy $A(p)$ of a network trained at corruption strength $p$.

\paragraph{Additive channel.}
We also consider an additive noise channel with strength $\sigma$:
\[
\tilde{x}_i = \mathrm{clip}\big(x_i + \epsilon_i,\,0,\,1\big), \quad
\epsilon_i \sim \mathrm{Uniform}(-\sigma/2,\sigma/2),
\]
with independent noise across pixels.

\paragraph{Binary replacement channel.}
For binarized MNIST, we threshold each pixel as $x_i \leftarrow \mathbb{I}[x_i \ge 1/2]$.
The binary corruption replaces each pixel with a Bernoulli(1/2) value with probability $p$:
\[
(\mathcal{B}_p(x))_i =
\begin{cases}
b_i, & \text{with probability } p,\\
x_i, & \text{with probability } 1-p,
\end{cases}
\quad b_i \sim \mathrm{Bernoulli}(1/2).
\]

\section*{Corruption as Attenuation + Noise}
Let $M_i \sim \mathrm{Bernoulli}(p)$ and $u_i \sim \mathrm{Uniform}(0,1)$, independent. Then
\[
\tilde{x} = (1-M) \odot x + M \odot u.
\]
Conditioned on $x$,
\[
\mathbb{E}[\tilde{x}\mid x] = (1-p) x + \tfrac{p}{2}\mathbf{1}, \quad
\mathrm{Var}(\tilde{x}_i \mid x) = p(1-p)(x_i - \tfrac{1}{2})^2 + \tfrac{p}{12}.
\]
So corruption both shrinks the signal by $(1-p)$ and injects noise of scale $\sqrt{p}$.

For a generic scalar score function $s(x)$ (e.g., a logit margin), a first-order expansion gives
\[
s(\tilde{x}) \approx s(x) + \nabla_x s(x)^\top (\tilde{x} - x).
\]
This is a local but exact linearization of the trained network. The gradient
$g(x) = \nabla_x s(x)$ is a measurable sensitivity vector.

\section*{Margin Criterion for a Sharp Drop}
Define the margin for example $(x,y)$ as
\[
\gamma(x) = f_y(x) - \max_{k\ne y} f_k(x),
\]
where $f_k$ is the logit for class $k$. A sufficient condition for label flip is
$\gamma(\tilde{x}) < 0$. Using the linearization above,
\[
\Delta \gamma \approx g(x)^\top (\tilde{x} - x).
\]
Because the corruption is iid across pixels, $\Delta \gamma$ concentrates with variance
\[
\mathrm{Var}(\Delta \gamma \mid x) \approx \sum_i g_i(x)^2 \, \mathrm{Var}(\tilde{x}_i \mid x).
\]
This suggests a threshold when typical fluctuations match the clean margin:
\[
\gamma(x) \approx c \, \sqrt{\mathrm{Var}(\Delta \gamma \mid x)}.
\]
Aggregating over the data distribution yields a population-level crossover $p^\star$.
As model size increases, the margin distribution can shift and sharpen, causing a steeper
drop in accuracy as $p$ passes $p^\star$.

\section*{Finite-Size Scaling Hypothesis}
Let $A(p)$ be the test accuracy when training with corruption $p$. We hypothesize:
\begin{itemize}
  \item \textbf{Shift:} the midpoint $p^\star$ of the accuracy drop increases with model size
  or data size, reflecting improved margins.
  \item \textbf{Sharpening:} the slope $|A'(p^\star)|$ increases with size, producing an
  apparently ``critical'' knee.
  \item \textbf{Collapse:} when plotted against an effective SNR proxy (e.g.,
  $(1-p)/\sqrt{p}$ or a measured margin-to-noise ratio), curves for different sizes align.
\end{itemize}
This is a finite-size crossover that can mimic a phase transition in the large-system limit.

\section*{Testable Predictions}
\begin{enumerate}
  \item Fit $A(p)$ with a sigmoid to estimate $p^\star$ and the slope; study scaling vs. width.
  \item Compute $g(x)=\nabla_x \gamma(x)$ on a held-out set; test whether
  $\gamma(x)/\sqrt{\mathrm{Var}(\Delta \gamma \mid x)}$ predicts failures.
  \item Compare MLP vs. CNN: CNNs should tolerate larger $p^\star$ due to inductive bias.
  \item Test curve collapse using an empirical SNR proxy derived from margins and gradients.
\end{enumerate}

\section*{RBM Baseline (Practical Reference)}
As an additional comparison point, one can train a Bernoulli RBM on corrupted inputs and
stack a logistic regression classifier on the learned features. This provides a classical
unsupervised baseline to contrast with modern supervised networks under the same corruption
channel.

\section*{RBM Baseline (Mathematical Detail)}
Let $v \in \{0,1\}^D$ denote a visible binary vector (e.g., a binarized or rescaled image)
and $h \in \{0,1\}^H$ the hidden units. A Bernoulli RBM defines an energy:
\[
E(v,h) = -b^\top v - c^\top h - v^\top W h,
\]
with parameters $(W,b,c)$. The joint distribution is
\[
p(v,h) = \frac{1}{Z} \exp(-E(v,h)), \quad Z = \sum_{v,h}\exp(-E(v,h)).
\]
The conditional distributions factorize:
\[
p(h_j=1\mid v) = \sigma\!\left(c_j + \sum_i W_{ij} v_i\right), \quad
p(v_i=1\mid h) = \sigma\!\left(b_i + \sum_j W_{ij} h_j\right),
\]
where $\sigma(x) = (1+e^{-x})^{-1}$.

\paragraph{Maximum likelihood.}
For a dataset $\{v^{(n)}\}_{n=1}^N$, the log-likelihood gradient is
\[
\frac{\partial \log p(v)}{\partial W}
= \mathbb{E}_{p(h\mid v)}[v h^\top] - \mathbb{E}_{p(v,h)}[v h^\top],
\]
with analogous expressions for $b,c$. The second term (model expectation) is intractable.

\paragraph{Contrastive divergence (CD-$k$).}
CD approximates the model term by running a short Gibbs chain:
\[
v^{(0)} = v, \quad h^{(t)} \sim p(h\mid v^{(t)}), \quad
v^{(t+1)} \sim p(v\mid h^{(t)}), \quad t=0,\ldots,k-1.
\]
The update uses
\[
\Delta W \propto v^{(0)} {h^{(0)}}^\top - v^{(k)} {h^{(k)}}^\top.
\]
The \emph{epoch} analogue for the RBM is the number of full passes over the dataset,
denoted by \texttt{rbm\_n\_iter} in our implementation. The RBM ``width'' is the number
of hidden units $H$ (called \texttt{n\_components} in scikit-learn).

\paragraph{Classification via logistic regression.}
After unsupervised training, one can map inputs to hidden activations
$\phi(v) = \mathbb{E}[h\mid v]$ and train a linear classifier:
\[
p_\theta(y\mid v) = \mathrm{softmax}(A\,\phi(v) + d),
\]
with cross-entropy loss
\[
L(\theta) = -\frac{1}{N}\sum_{n=1}^N \log p_\theta(y^{(n)}\mid v^{(n)}).
\]
The classifier training iterations are governed by \texttt{rbm\_classifier\_max\_iter}.

\paragraph{Corruption channel.}
When the input is corrupted by the replacement or additive channel, the RBM is trained on
$\tilde{v} = \mathcal{C}(v)$; the same corruption is applied at test time for evaluating
accuracy vs.\ corruption strength.

\section*{Empirical NTK Diagnostics (Finite Width)}
For a network $f(x;\theta)$, the empirical NTK at parameters $\theta$ is
\[
K_\theta(x,x') = \nabla_\theta f(x;\theta)^\top \nabla_\theta f(x';\theta).
\]
At finite width, $K_\theta$ depends on the initialization and evolves with training.
We compute $K_\theta$ after training on corrupted data and evaluate it on a small
subset of inputs (typically a random subset of the test set).

\paragraph{Effective rank and spectral entropy.}
Let $\{\lambda_i\}$ be the eigenvalues of $K_\theta$ and define
$\tilde{\lambda}_i = \lambda_i / \sum_j \lambda_j$. The spectral entropy is
\[
H = -\sum_i \tilde{\lambda}_i \log \tilde{\lambda}_i,
\]
and the effective rank is
\[
r_{\mathrm{eff}} = \exp(H).
\]
We track $r_{\mathrm{eff}}$ (and $H$) versus corruption strength to quantify how the
NTK spectrum flattens or concentrates as $p$ increases. We also record eigenvalue
histograms at selected $p$ values to visualize the full spectral distribution.

\end{document}
